---
title: "Biostat 203B Homework 2"
subtitle: Due Feb 9 @ 11:59PM
author: "Jiachen Ai, UID:206182615"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
knitr:
  opts_chunk: 
    cache: false    
    echo: true
    fig.align: 'center'
    fig.width: 6
    fig.height: 4
    message: FALSE
execute:
  eval: false    
---

Display machine information for reproducibility:

```{r}
#| eval: true
sessionInfo()
```

Load necessary libraries (you can add more as needed).

```{r setup}
#| eval: true
library(arrow)
library(data.table)
library(memuse)
library(pryr)
library(R.utils)
library(tidyverse)
library(readr)
library(dplyr)
library(duckdb)
library(DBI)
```

Display memory information of your computer

```{r}
#| eval: true
memuse::Sys.meminfo()
```

In this exercise, we explore various tools for ingesting the [MIMIC-IV](https://mimic.mit.edu/docs/iv/) data introduced in [homework 1](https://ucla-biostat-203b.github.io/2024winter/hw/hw1/hw1.html).

Display the contents of MIMIC `hosp` and `icu` data folders:

```{bash}
#| eval: true
ls -l ~/mimic/hosp/
```

```{bash}
#| eval: true
ls -l ~/mimic/icu/
```

## Q1. `read.csv` (base R) vs `read_csv` (tidyverse) vs `fread` (data.table)

### Q1.1 Speed, memory, and data types

There are quite a few utilities in R for reading plain text data files. Let us test the speed of reading a moderate sized compressed csv file, `admissions.csv.gz`, by three functions: `read.csv` in base R, `read_csv` in tidyverse, and `fread` in the data.table package.

Which function is fastest? Is there difference in the (default) parsed data types? How much memory does each resultant dataframe or tibble use? (Hint: `system.time` measures run times; `pryr::object_size` measures memory usage.)

**Answer**

First, we will read the `admissions.csv.gz` file using `read.csv`, `read_csv`, and `fread` functions, and compare the speed respectively.

```{r}
#| eval: true
mimic_path <- "~/mimic/hosp/"
# reading time for read.csv
system.time(data_read.csv <- read.csv(
  str_c(mimic_path,"admissions.csv.gz")))
# reading time for read_csv
system.time(data_read_csv <- read_csv(
  str_c(mimic_path,"admissions.csv.gz")))
# reading time for fread
system.time(data_fread <- fread(
  str_c(mimic_path,"admissions.csv.gz")))
```

Then, we will compare the memory usage.

```{r}
#| eval: true
# memory usage for read.csv
pryr::object_size(data_read.csv)
# memory usage for read_csv
pryr::object_size(data_read_csv)
# memory usage for fread
pryr::object_size(data_fread)
```

Finally, we will compare the data types.

```{r}
#| eval: true
# data types for read.csv
str(data_read.csv)
```

```{r}
#| eval: true
# data types for read_csv
str(data_read_csv)
```

```{r}
#| eval: true
# data types for fread
str(data_read_csv)
```

In terms of the speed and based on the user time, `fread` is the fastest, followed by `read_csv`, and `read.csv` is the slowest (according to the elapsed time).

In terms of the memory usage of the resultant dataframe or tibble, `fread` uses the least memory, followed by `read_csv`, and `read.csv` uses the most memory.

In terms of the default parsed data types, `fread` and `read_csv` are similar if "double" and "integer" are categorized as "numeric" data type; However, `read.csv` is different in some variables' data type.

### Q1.2 User-supplied data types

Re-ingest `admissions.csv.gz` by indicating appropriate column data types in `read_csv`. Does the run time change? How much memory does the result tibble use? (Hint: `col_types` argument in `read_csv`.)

**Answer**

I indicated appropriate column data types in `read_csv` by looking at the original data types in the `admissions.csv.gz` file.

```{r}
#| eval: true
system.time(data_read_csv <- read_csv(
  str_c(mimic_path,"admissions.csv.gz"), 
  col_types = "nnTTTfcffffffTTn"))
pryr::object_size(data_read_csv)
```

Yes. The run time became shorter because the data types of the variables are specified according to the data types in the original data. The memory that the resultant tibble uses is 43.24 MB which is smaller than before.

## Q2. Ingest big data files

<p align="center">

<img src="./bigfile.png" width="50%"/>

</p>

Let us focus on a bigger file, `labevents.csv.gz`, which is about 125x bigger than `admissions.csv.gz`.

```{bash}
#| eval: true
ls -l ~/mimic/hosp/labevents.csv.gz
```

Display the first 10 lines of this file.

```{bash}
#| eval: true
zcat < ~/mimic/hosp/labevents.csv.gz | head -10
```

### Q2.1 Ingest `labevents.csv.gz` by `read_csv`

<p align="center">

<img src="./readr_logo.png" width="20%"/>

</p>

Try to ingest `labevents.csv.gz` using `read_csv`. What happens? If it takes more than 5 minutes on your computer, then abort the program and report your findings.

**Answer**

I tried to read the file using `read_csv` function as follows:

```{r}
#| eval: false
read_csv(str_c("~/mimic/hosp/","labevents.csv.gz"))
```

I spent more than 5 minutes to read the file, but it was not finished. So, I aborted the program. The reason is that the size of the file is quite large (1.94GB), which means there is a significant amount of data to read and process. And since the file is compressed (labevents.csv.gz), it needs to be decompressed before it can be read. Decompressing large files can take time. Also, reading large files can require a significant amount of memory, and my system has limited memory.

### Q2.2 Ingest selected columns of `labevents.csv.gz` by `read_csv`

Try to ingest only columns `subject_id`, `itemid`, `charttime`, and `valuenum` in `labevents.csv.gz` using `read_csv`. Does this solve the ingestion issue? (Hint: `col_select` argument in `read_csv`.)

**Answer:** I tried to read the file using `read_csv` function with `col_select` argument as follows:

```{r}
#| eval: true
read_csv("~/mimic/hosp/labevents.csv.gz", 
         col_select = c("subject_id", 
                        "itemid", 
                        "charttime", 
                        "valuenum"))
```

It doesn't solve the ingestion issue. The file was read more than 5 mins. Also, the file is still quite large, and the memory usage is high.

### Q2.3 Ingest subset of `labevents.csv.gz`

<p align="center">

<img src="./linux_logo.png" width="20%"/>

</p>

Our first strategy to handle this big data file is to make a subset of the `labevents` data. Read the [MIMIC documentation](https://mimic.mit.edu/docs/iv/modules/hosp/labevents/) for the content in data file `labevents.csv`.

In later exercises, we will only be interested in the following lab items: creatinine (50912), potassium (50971), sodium (50983), chloride (50902), bicarbonate (50882), hematocrit (51221), white blood cell count (51301), and glucose (50931) and the following columns: `subject_id`, `itemid`, `charttime`, `valuenum`. Write a Bash command to extract these columns and rows from `labevents.csv.gz` and save the result to a new file `labevents_filtered.csv.gz` in the current working directory. (Hint: use `zcat <` to pipe the output of `labevents.csv.gz` to `awk` and then to `gzip` to compress the output. To save render time, put `#| eval: false` at the beginning of this code chunk.)

Display the first 10 lines of the new file `labevents_filtered.csv.gz`. How many lines are in this new file? How long does it take `read_csv` to ingest `labevents_filtered.csv.gz`?

**Answer**

First, displaying the first 10 lines of the `labevents.csv.gz` file to know the structure of the file and the sequence of the columns.

```{bash}
#| eval: true
zcat < ~/mimic/hosp/labevents.csv.gz | head
```

Then, extract specific columns and rows from the `labevents.csv.gz` file and save the result to a new file `labevents_filtered.csv.gz`

```{bash}
#| eval: false
zcat < ~/mimic/hosp/labevents.csv.gz | \
awk -F ',' '{
    if ($5 ~ /^(50912|50971|50983|50902|50882|51221|51301|50931)$/) \
        print $2,$5,$7,$10
    }' | \
gzip > ~/mimic/hosp/labevents_filtered.csv.gz
```

Display the first 10 lines of the new file `labevents_filtered.csv.gz`

```{bash}
#| eval: true
zcat < ~/mimic/hosp/labevents_filtered.csv.gz | head -n 10
```

Calculate the number of lines in the new file `labevents_filtered.csv.gz`. There are 24855909 lines in this new file.

```{bash}
#| eval: true
zcat < ~/mimic/hosp/labevents_filtered.csv.gz | wc -l
```

Finally, to measure how long it takes to ingest `labevents_filtered.csv.gz`. The following code chunk shows the time it takes to read the file.

```{r}
#| eval: true
system.time({
  data <- read_csv("~/mimic/hosp/labevents_filtered.csv.gz")
})
```

### Q2.4 Ingest `labevents.csv` by Apache Arrow

<p align="center">

<img src="./arrow_logo.png" width="30%"/>

</p>

Our second strategy is to use [Apache Arrow](https://arrow.apache.org/) for larger-than-memory data analytics. Unfortunately Arrow does not work with gz files directly. First decompress `labevents.csv.gz` to `labevents.csv` and put it in the current working directory. To save render time, put `#| eval: false` at the beginning of this code chunk.

Then use [`arrow::open_dataset`](https://arrow.apache.org/docs/r/reference/open_dataset.html) to ingest `labevents.csv`, select columns, and filter `itemid` as in Q2.3. How long does the ingest+select+filter process take? Display the number of rows and the first 10 rows of the result tibble, and make sure they match those in Q2.3. (Hint: use `dplyr` verbs for selecting columns and filtering rows.)

Write a few sentences to explain what is Apache Arrow. Imagine you want to explain it to a layman in an elevator.

**Answer**

First, decompressing the `labevents.csv.gz file`

```{r}
#| eval: false
system("gzip -d -k ~/mimic/hosp/labevents.csv.gz")
```

Then, using `arrow::open_dataset` to ingest `labevents.csv`, select columns, and filter `itemid` as in Q2.3.

```{r}
#| eval: true
file_path <- "~/mimic/hosp/labevents.csv"

#ingest+select+filter process and measure the time it takes
system.time(labevents <- arrow::open_dataset(file_path, 
                                             format = "csv") %>% 
  dplyr::select(subject_id, itemid, charttime, valuenum) %>% 
  dplyr::filter(itemid %in% c(50912, 50971, 50983, 50902, 
                              50882, 51221, 51301, 50931)) %>% 
  collect())
```

And then, to display the number of rows in the result tibble.

```{r}
#| eval: true
#display the number of rows in the result tibble
nrow(labevents)
```

The time it takes to ingest+select+filter process is shown in the output above. The number of rows in the result tibble is 24855909.

Then, to display the first 10 rows of the result tibble.

The content in `subject_id`, `itemid`, and `valuenum` match those in Q2.3. However, the `charttime` column is in a different format, which is because the code will automatically recognize the input time as a UTC time and convert it to the local time zone.

```{r}
#| eval: true
head(labevents, 10)
```

**Explanation of Apache Arrow**: Apache Arrow is like a universal language for data that allows different software programs to communicate and share data more efficiently. It's a technology that makes it easier for different systems to understand and work with large amounts of data, regardless of the programming language they are written in. So, it's like a common ground where data can be exchanged and understood by various applications, making data analysis and processing faster and more seamless, just like how people from different countries can communicate more effectively using a common language.

### Q2.5 Compress `labevents.csv` to Parquet format and ingest/select/filter

<p align="center">

<img src="./parquet_logo.png" width="30%"/>

</p>

Re-write the csv file `labevents.csv` in the binary Parquet format (Hint: [`arrow::write_dataset`](https://arrow.apache.org/docs/r/reference/write_dataset.html).) How large is the Parquet file(s)? How long does the ingest+select+filter process of the Parquet file(s) take? Display the number of rows and the first 10 rows of the result tibble and make sure they match those in Q2.3. (Hint: use `dplyr` verbs for selecting columns and filtering rows.)

Write a few sentences to explain what is the Parquet format. Imagine you want to explain it to a layman in an elevator.

**Answer**

First, re-writing the csv file `labevents.csv` in the binary Parquet format.

```{r}
#| eval: true
# Define the file paths
parquet_file_path <- "~/mimic/hosp/labevents.parquet"

# Rewrite the CSV file in Parquet format    
arrow::write_dataset(arrow::open_dataset(file_path, format = "csv"), 
                     parquet_file_path, format = "parquet")

```

Measuring the size of the Parquet file. And the size of the Parquet file is 1.9GB.

```{bash}
#| eval: true
#measure the size of the Parquet file
du -h ~/mimic/hosp/labevents.parquet
```

Then, measuring the time taken for ingest+select+filter process of the Parquet file.

```{r}
#| eval: true
system.time({
  parquet_dataset <- 
    arrow::open_dataset(parquet_file_path, format = "parquet") %>%
    dplyr::select(subject_id, itemid, charttime, valuenum) %>%
    dplyr::filter(itemid %in% c(50912, 50971, 50983, 50902, 
                                50882, 51221, 51301, 50931)) %>%
    collect()
})
```

And measuring the number of rows in the result tibble.

```{r}
#| eval: true
nrow(parquet_dataset)
```

The time it takes to ingest+select+filter process of the Parquet file is shown in the output above. And the number of rows in the result tibble is 24855909.

Then, to display the first 10 rows of the result tibble.

The content in `subject_id`, `itemid`, and `valuenum` match those in Q2.3. However, the `charttime` column is in a different format, which is because the code will automatically recognize the input time as a UTC time and convert it to the local time zone.

```{r}
#| eval: true
#display the first 10 rows of the result tibble
head(parquet_dataset, 10)
```

**Explanation of Parquet format**: Parquet is a file format for storing data that is designed to be efficient and optimized for use with big data processing frameworks. It's like a special type of container that organizes data in a highly compressed and columnar format, making it easier for computers to read and process large amounts of data quickly. Think of it as a way to pack and organize data in a smart and efficient way, similar to how a well-organized filing cabinet makes it easy to find and access documents quickly.

### Q2.6 DuckDB

<p align="center">

<img src="./duckdb_logo.png" width="20%"/>

</p>

Ingest the Parquet file, convert it to a DuckDB table by [`arrow::to_duckdb`](https://arrow.apache.org/docs/r/reference/to_duckdb.html), select columns, and filter rows as in Q2.5. How long does the ingest+convert+select+filter process take? Display the number of rows and the first 10 rows of the result tibble and make sure they match those in Q2.3. (Hint: use `dplyr` verbs for selecting columns and filtering rows.)

Write a few sentences to explain what is DuckDB. Imagine you want to explain it to a layman in an elevator.

**Answer**

First, ingesting the Parquet file, converting it to a DuckDB table. And measuring the time taken for the ingest+convert+select+filter process.

```{r}
#| eval: true
parquet_file_path_1 <- "~/mimic/hosp/labevents.parquet/part-0.parquet"
# measure the time taken for the ingest+convert+select+filter process
system.time({
  parquet_dataset <- arrow::read_parquet(parquet_file_path_1, 
                                         format = "parquet")
  duckdb_table <- arrow::to_duckdb(parquet_dataset)
  result <- duckdb_table %>%
    dplyr::filter(itemid %in% c(50912, 50971, 50983, 50902, 
                                50882, 51221, 51301, 50931)) %>%
    dplyr::select(subject_id, itemid, charttime, valuenum) %>%
    collect()
})
```

Then, measuring the number of rows in the result tibble.

```{r}
#| eval: true
nrow(result)
```

The time it takes to ingest+convert+select+filter process is shown in the output above. And the number of rows in the result tibble is 24855909.

Then, to display the first 10 rows of the result tibble.

The content in `subject_id`, `itemid`, and `valuenum` match those in Q2.3. However, the `charttime` column is in a different format, which is because the code will automatically recognize the input time as a UTC time and convert it to the local time zone.

```{r}
#| eval: true
# Display the first 10 rows
head(result, 10)
```

**Explanation of DuckDB**: DuckDB is like a digital filing cabinet for data. It's a special kind of software that helps organize and store large amounts of data in a way that makes it easy to find and work with. Think of it as a super-efficient librarian that keeps all your data neatly organized and quickly accessible. It's especially useful for tasks like analyzing data, running calculations, and generating reports, making it a valuable tool for businesses and researchers who deal with lots of data.

Fast
DuckDB is designed to support analytical query workloads, also known as online analytical processing (OLAP). These workloads are characterized by complex, relatively long-running queries that process significant portions of the stored dataset, for example aggregations over entire tables or joins between several large tables. Changes to the data are expected to be rather large-scale as well, with several rows being appended, or large portions of tables being changed or added at the same time.

To efficiently support this workload, it is critical to reduce the amount of CPU cycles that are expended per individual value. The state of the art in data management to achieve this are either vectorized or just-in-time query execution engines. DuckDB contains a columnar-vectorized query execution engine, where queries are still interpreted, but a large batch of values (a “vector”) are processed in one operation. This greatly reduces overhead present in traditional systems such as PostgreSQL, MySQL or SQLite which process each row sequentially. Vectorized query execution leads to far better performance in OLAP queries.



## Q3. Ingest and filter `chartevents.csv.gz`

[`chartevents.csv.gz`](https://mimic.mit.edu/docs/iv/modules/icu/chartevents/) contains all the charted data available for a patient. During their ICU stay, the primary repository of a patient's information is their electronic chart. The `itemid` variable indicates a single measurement type in the database. The `value` variable is the value measured for `itemid`. The first 10 lines of `chartevents.csv.gz` are

```{bash}
#| eval: true
zcat < ~/mimic/icu/chartevents.csv.gz | head -10
```

[`d_items.csv.gz`](https://mimic.mit.edu/docs/iv/modules/icu/d_items/) is the dictionary for the `itemid` in `chartevents.csv.gz`.

```{bash}
#| eval: true
zcat < ~/mimic/icu/d_items.csv.gz | head -10
```

In later exercises, we are interested in the vitals for ICU patients: heart rate (220045), mean non-invasive blood pressure (220181), systolic non-invasive blood pressure (220179), body temperature in Fahrenheit (223761), and respiratory rate (220210). Retrieve a subset of `chartevents.csv.gz` only containing these items, using the favorite method you learnt in Q2.

Document the steps and show code. Display the number of rows and the first 10 rows of the result tibble.

**Answer**

First, unzipping the file `chartevents.csv.gz`

```{r}
#| eval: false
# Unzip the file
system("gzip -d -k ~/mimic/icu/chartevents.csv.gz")
```

Then, I want to use the `arrow::open_dataset` method to ingest the CSV file, select columns, and filter rows based on `itemid`, because the speed of `arrow::open_dataset` is fast and it can handle large datasets efficiently.

```{r}
#| eval: true
file_path_3 <- "~/mimic/icu/chartevents.csv"

chartevents <- arrow::open_dataset(file_path_3, format = "csv") %>%
  dplyr::filter(itemid %in% c(220045, 220181, 220179, 
                              223761, 220210)) %>%
  dplyr::select(subject_id, itemid, charttime, value) %>%
  collect()
```

Finally, displaying the number of rows and the first 10 rows of the result tibble. There are 22502319 rows in the result tibble.

```{r}
#| eval: true
nrow(chartevents)
```

Display the first 10 rows of the result.

```{r}
#| eval: true
head(chartevents, 10)
```
