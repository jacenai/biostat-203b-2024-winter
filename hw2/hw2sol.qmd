---
title: "Biostat 203B Homework 2"
subtitle: Due Feb 9 @ 11:59PM
author: "Jiachen Ai, UID:206182615"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
knitr:
  opts_chunk: 
    cache: false    
    echo: true
    fig.align: 'center'
    fig.width: 6
    fig.height: 4
    message: FALSE
execute:
  eval: false    
---

Display machine information for reproducibility:

```{r}
#| eval: true
sessionInfo()
```

Load necessary libraries (you can add more as needed).

```{r setup}
#| eval: true
library(arrow)
library(data.table)
library(memuse)
library(pryr)
library(R.utils)
library(tidyverse)
library(readr)
library(dplyr)
library(duckdb)
library(DBI)
```

Display memory information of your computer

```{r}
#| eval: true
memuse::Sys.meminfo()
```

In this exercise, we explore various tools for ingesting the [MIMIC-IV](https://mimic.mit.edu/docs/iv/) data introduced in [homework 1](https://ucla-biostat-203b.github.io/2024winter/hw/hw1/hw1.html).

Display the contents of MIMIC `hosp` and `icu` data folders:

```{bash}
#| eval: true
ls -l ~/mimic/hosp/
```

```{bash}
#| eval: true
ls -l ~/mimic/icu/
```

## Q1. `read.csv` (base R) vs `read_csv` (tidyverse) vs `fread` (data.table)

### Q1.1 Speed, memory, and data types

There are quite a few utilities in R for reading plain text data files. Let us test the speed of reading a moderate sized compressed csv file, `admissions.csv.gz`, by three functions: `read.csv` in base R, `read_csv` in tidyverse, and `fread` in the data.table package.

Which function is fastest? Is there difference in the (default) parsed data types? How much memory does each resultant dataframe or tibble use? (Hint: `system.time` measures run times; `pryr::object_size` measures memory usage.)

**Answer**

First, we will read the `admissions.csv.gz` file using `read.csv`, `read_csv`, and `fread` functions, and compare the speed respectively.

```{r}
#| eval: true
mimic_path <- "~/mimic/hosp/"
# reading time for read.csv
system.time(data_read.csv <- read.csv(
  str_c(mimic_path,"admissions.csv.gz")))
# reading time for read_csv
system.time(data_read_csv <- read_csv(
  str_c(mimic_path,"admissions.csv.gz")))
# reading time for fread
system.time(data_fread <- fread(
  str_c(mimic_path,"admissions.csv.gz")))
```

Then, we will compare the memory usage.

```{r}
#| eval: true
# memory usage for read.csv
pryr::object_size(data_read.csv)
# memory usage for read_csv
pryr::object_size(data_read_csv)
# memory usage for fread
pryr::object_size(data_fread)
```

Finally, we will compare the data types.

```{r}
#| eval: true
# data types for read.csv
str(data_read.csv)
```

```{r}
#| eval: true
# data types for read_csv
str(data_read_csv)
```

```{r}
#| eval: true
# data types for fread
str(data_read_csv)
```

In terms of the speed and based on the user time, `fread` is the fastest, followed by `read_csv`, and `read.csv` is the slowest. (according to the user time)

In terms of the memory usage of the resultant dataframe or tibble, `fread` uses the least memory, followed by `read_csv`, and `read.csv` uses the most memory.

In terms of the default parsed data types, `fread` and `read_csv` are similar if "double" and "integer" are categorized as "numeric" data type; However, `read.csv` is different in some variables' data type.

### Q1.2 User-supplied data types

Re-ingest `admissions.csv.gz` by indicating appropriate column data types in `read_csv`. Does the run time change? How much memory does the result tibble use? (Hint: `col_types` argument in `read_csv`.)

**Answer**

```{r}
#| eval: true
system.time(data_read_csv <- read_csv(
  str_c(mimic_path,"admissions.csv.gz"), 
  col_types = "nnTTTfcffffffTTn"))
pryr::object_size(data_read_csv)
```

Yes. The run time became shorter because the data types of the variables are specified according to the data types in the original data. The memory that the resultant tibble uses is 43.24 MB which is smaller than before.

## Q2. Ingest big data files

<p align="center">

<img src="./bigfile.png" width="50%"/>

</p>

Let us focus on a bigger file, `labevents.csv.gz`, which is about 125x bigger than `admissions.csv.gz`.

```{bash}
#| eval: true
ls -l ~/mimic/hosp/labevents.csv.gz
```

Display the first 10 lines of this file.

```{bash}
#| eval: true
zcat < ~/mimic/hosp/labevents.csv.gz | head -10
```

### Q2.1 Ingest `labevents.csv.gz` by `read_csv`

<p align="center">

<img src="./readr_logo.png" width="20%"/>

</p>

Try to ingest `labevents.csv.gz` using `read_csv`. What happens? If it takes more than 5 minutes on your computer, then abort the program and report your findings. 
**Answer**

```{r}
#| eval: false
read_csv(str_c("~/mimic/hosp/","labevents.csv.gz"))
```

I spent more than 5 minutes to read the file, but it was not finished. So, I aborted the program. The reason is that the size of the file is quite large (1.94GB), which means there is a significant amount of data to read and process. And since the file is compressed (labevents.csv.gz), it needs to be decompressed before it can be read. Decompressing large files can take time. Also, reading large files can require a significant amount of memory, and my system has limited memory.

### Q2.2 Ingest selected columns of `labevents.csv.gz` by `read_csv`

Try to ingest only columns `subject_id`, `itemid`, `charttime`, and `valuenum` in `labevents.csv.gz` using `read_csv`. Does this solve the ingestion issue? (Hint: `col_select` argument in `read_csv`.)

```{r}
#| eval: true
read_csv("~/mimic/hosp/labevents.csv.gz", 
         col_select = c("subject_id", 
                        "itemid", 
                        "charttime", 
                        "valuenum"))
```

### Q2.3 Ingest subset of `labevents.csv.gz`

<p align="center">

<img src="./linux_logo.png" width="20%"/>

</p>

Our first strategy to handle this big data file is to make a subset of the `labevents` data. Read the [MIMIC documentation](https://mimic.mit.edu/docs/iv/modules/hosp/labevents/) for the content in data file `labevents.csv`.

In later exercises, we will only be interested in the following lab items: creatinine (50912), potassium (50971), sodium (50983), chloride (50902), bicarbonate (50882), hematocrit (51221), white blood cell count (51301), and glucose (50931) and the following columns: `subject_id`, `itemid`, `charttime`, `valuenum`. Write a Bash command to extract these columns and rows from `labevents.csv.gz` and save the result to a new file `labevents_filtered.csv.gz` in the current working directory. (Hint: use `zcat <` to pipe the output of `labevents.csv.gz` to `awk` and then to `gzip` to compress the output. To save render time, put `#| eval: false` at the beginning of this code chunk.)

Display the first 10 lines of the new file `labevents_filtered.csv.gz`. How many lines are in this new file? How long does it take `read_csv` to ingest `labevents_filtered.csv.gz`?

**Answer**

First, displaying the first 10 lines of the `labevents.csv.gz` file to know the structure of the file and the sequence of the columns.

```{bash}
#| eval: false
zcat < ~/mimic/hosp/labevents.csv.gz | head
```

To extract specific columns and rows from the `labevents.csv.gz` file and save the result to a new file `labevents_filtered.csv.gz`

```{bash}
#| eval: false
zcat < ~/mimic/hosp/labevents.csv.gz | \
awk -F ',' '{
    if ($5 ~ /^(50912|50971|50983|50902|50882|51221|51301|50931)$/) \
        print $2,$5,$7,$10
    }' | \
gzip > ~/mimic/hosp/labevents_filtered.csv.gz
```

To display the first 10 lines of the new file `labevents_filtered.csv.gz`

```{bash}
#| eval: true
zcat < ~/mimic/hosp/labevents_filtered.csv.gz | head -n 10
```

To calculate the number of lines in the new file `labevents_filtered.csv.gz`. There are 24855909 lines in this new file.

```{bash}
#| eval: true
zcat < ~/mimic/hosp/labevents_filtered.csv.gz | wc -l
```

Finally, to measure how long it takes to ingest `labevents_filtered.csv.gz`. It takes around 216 seconds to ingest the new file `labevents_filtered.csv.gz`.

```{r}
#| eval: true
system.time({
  data <- read_csv("~/mimic/hosp/labevents_filtered.csv.gz")
})
```

### Q2.4 Ingest `labevents.csv` by Apache Arrow

<p align="center">

<img src="./arrow_logo.png" width="30%"/>

</p>

Our second strategy is to use [Apache Arrow](https://arrow.apache.org/) for larger-than-memory data analytics. Unfortunately Arrow does not work with gz files directly. First decompress `labevents.csv.gz` to `labevents.csv` and put it in the current working directory. To save render time, put `#| eval: false` at the beginning of this code chunk.

Then use [`arrow::open_dataset`](https://arrow.apache.org/docs/r/reference/open_dataset.html) to ingest `labevents.csv`, select columns, and filter `itemid` as in Q2.3. How long does the ingest+select+filter process take? Display the number of rows and the first 10 rows of the result tibble, and make sure they match those in Q2.3. (Hint: use `dplyr` verbs for selecting columns and filtering rows.)

Write a few sentences to explain what is Apache Arrow. Imagine you want to explain it to a layman in an elevator.

**Answer**

First, decompressing the `labevents.csv.gz file`

```{r}
#| eval: false
system("gzip -d -k ~/mimic/hosp/labevents.csv.gz")
```

Then, using `arrow::open_dataset` to ingest `labevents.csv`, select columns, and filter `itemid` as in Q2.3.

```{r}
#| eval: true
file_path <- "~/mimic/hosp/labevents.csv"

start_time <- Sys.time() # Record the start time

labevents <- arrow::open_dataset(file_path, format = "csv") %>%
  filter(itemid %in% c(50912, 50971, 50983, 50902, 
                       50882, 51221, 51301, 50931)) %>%
  select(subject_id, itemid, charttime, valuenum)

end_time <- Sys.time()  # Record the end time

execution_time <- end_time - start_time
execution_time
```

The ingest+select+filter process takes around 0.05140901 secs. The number of rows in the result tibble is 24855909. Then, to display the first 10 rows of the result tibble and they match those in Q2.3.

```{r}
#| eval: true
# Convert the Arrow object to a data frame
labevents_df_2.4 <- as.data.frame(labevents)
# Display the number of rows
cat("Number of rows:", nrow(labevents_df_2.4), "\n")
# Display the first 10 rows
head(labevents_df_2.4, 10)
```

**Explanation of Apache Arrow**: Apache Arrow is like a universal language for data that allows different software programs to communicate and share data more efficiently. It's a technology that makes it easier for different systems to understand and work with large amounts of data, regardless of the programming language they are written in. So, it's like a common ground where data can be exchanged and understood by various applications, making data analysis and processing faster and more seamless, just like how people from different countries can communicate more effectively using a common language.

### Q2.5 Compress `labevents.csv` to Parquet format and ingest/select/filter

<p align="center">

<img src="./parquet_logo.png" width="30%"/>

</p>

Re-write the csv file `labevents.csv` in the binary Parquet format (Hint: [`arrow::write_dataset`](https://arrow.apache.org/docs/r/reference/write_dataset.html).) How large is the Parquet file(s)? How long does the ingest+select+filter process of the Parquet file(s) take? Display the number of rows and the first 10 rows of the result tibble and make sure they match those in Q2.3. (Hint: use `dplyr` verbs for selecting columns and filtering rows.)

Write a few sentences to explain what is the Parquet format. Imagine you want to explain it to a layman in an elevator.

**Answer**

First, re-writing the csv file `labevents.csv` in the binary Parquet format.

```{r}
#| eval: true
# Define the file paths
parquet_file_path <- "~/mimic/hosp/labevents.parquet"

# Rewrite the CSV file in Parquet format
arrow::write_dataset(arrow::open_dataset(file_path, format = "csv"), 
                     parquet_file_path, format = "parquet")

```

Then, measuring the time taken for ingest+select+filter process of the Parquet file.

```{r}
#| eval: true
start_time <- Sys.time() # Record the start time

parquet_dataset <- 
  arrow::open_dataset(parquet_file_path, format = "parquet") %>%
  filter(itemid %in% c(50912, 50971, 50983, 50902, 
                       50882, 51221, 51301, 50931)) %>%
  select(subject_id, itemid, charttime, valuenum)

end_time <- Sys.time()  # Record the end time
execution_time <- end_time - start_time
execution_time


```

The ingest+select+filter process of the Parquet file takes around 0.202384 secs. And the number of rows in the result tibble is 24855909. Then, to display the first 10 rows of the result tibble and their contents match those in Q2.3, even though the order of the rows may be different.

```{r}
#| eval: true
labevents_df_2.5 <- as.data.frame(parquet_dataset)
# Display the number of rows
num_rows <- nrow(labevents_df_2.5)
cat("Number of rows:", num_rows, "\n")
# Display the first 10 rows
head(labevents_df_2.5, 10)
```

**Explanation of Parquet format**: Parquet is a file format for storing data that is designed to be efficient and optimized for use with big data processing frameworks. It's like a special type of container that organizes data in a highly compressed and columnar format, making it easier for computers to read and process large amounts of data quickly. Think of it as a way to pack and organize data in a smart and efficient way, similar to how a well-organized filing cabinet makes it easy to find and access documents quickly.

### Q2.6 DuckDB

<p align="center">

<img src="./duckdb_logo.png" width="20%"/>

</p>

Ingest the Parquet file, convert it to a DuckDB table by [`arrow::to_duckdb`](https://arrow.apache.org/docs/r/reference/to_duckdb.html), select columns, and filter rows as in Q2.5. How long does the ingest+convert+select+filter process take? Display the number of rows and the first 10 rows of the result tibble and make sure they match those in Q2.3. (Hint: use `dplyr` verbs for selecting columns and filtering rows.)

Write a few sentences to explain what is DuckDB. Imagine you want to explain it to a layman in an elevator.

**Answer**

First, ingesting the Parquet file, converting it to a DuckDB table.

```{r}
#| eval: true
parquet_file_path_1 <- "~/mimic/hosp/labevents.parquet/part-0.parquet"

start_time <- Sys.time() # Record the start time

# Ingest the Parquet file and convert it to a DuckDB table
parquet_dataset <- arrow::read_parquet(parquet_file_path_1, 
                                       fomrat = "parquet")
duckdb_table <- arrow::to_duckdb(parquet_dataset)

# Select specific columns and filter rows based on itemid
result <- duckdb_table %>%
  dplyr::filter(itemid %in% c(50912, 50971, 50983, 50902, 
                              50882, 51221, 51301, 50931)) %>%
  dplyr::select(subject_id, itemid, charttime, valuenum)

end_time <- Sys.time() # Record the end time
execution_time <- end_time - start_time
execution_time

```

The ingest+convert+select+filter process takes around 38.68542 secs. And the number of rows in the result tibble is 24855909. Then, to display the first 10 rows of the result tibble and their contents match those in Q2.3, even though the order of the rows may be different.

```{r}
#| eval: true
labevents_df_2.6 <- as.data.frame(result)
# Display the number of rows
num_rows <- nrow(labevents_df_2.6)
cat("Number of rows:", num_rows, "\n")
# Display the first 10 rows
head(labevents_df_2.6, 10)
```

**Explanation of DuckDB**: DuckDB is like a digital filing cabinet for data. It's a special kind of software that helps organize and store large amounts of data in a way that makes it easy to find and work with. Think of it as a super-efficient librarian that keeps all your data neatly organized and quickly accessible. It's especially useful for tasks like analyzing data, running calculations, and generating reports, making it a valuable tool for businesses and researchers who deal with lots of data.

## Q3. Ingest and filter `chartevents.csv.gz`

[`chartevents.csv.gz`](https://mimic.mit.edu/docs/iv/modules/icu/chartevents/) contains all the charted data available for a patient. During their ICU stay, the primary repository of a patient's information is their electronic chart. The `itemid` variable indicates a single measurement type in the database. The `value` variable is the value measured for `itemid`. The first 10 lines of `chartevents.csv.gz` are

```{bash}
#| eval: true
zcat < ~/mimic/icu/chartevents.csv.gz | head -10
```

[`d_items.csv.gz`](https://mimic.mit.edu/docs/iv/modules/icu/d_items/) is the dictionary for the `itemid` in `chartevents.csv.gz`.

```{bash}
#| eval: true
zcat < ~/mimic/icu/d_items.csv.gz | head -10
```

In later exercises, we are interested in the vitals for ICU patients: heart rate (220045), mean non-invasive blood pressure (220181), systolic non-invasive blood pressure (220179), body temperature in Fahrenheit (223761), and respiratory rate (220210). Retrieve a subset of `chartevents.csv.gz` only containing these items, using the favorite method you learnt in Q2.

Document the steps and show code. Display the number of rows and the first 10 rows of the result tibble.

**Answer**

First, unzipping the file `chartevents.csv.gz`

```{r}
#| eval: false
# Unzip the file
system("gzip -d -k ~/mimic/icu/chartevents.csv.gz")
```

Then, ingesting the CSV file, selecting columns, and filtering rows based on `itemid`.

```{r}
#| eval: true
file_path_3 <- "~/mimic/icu/chartevents.csv"

chartevents <- arrow::open_dataset(file_path_3, format = "csv") %>%
  filter(itemid %in% c(220045, 220181, 220179, 223761, 
                       220210)) 
```

Finally, displaying the number of rows and the first 10 rows of the result tibble. There are 22502319 rows in the result tibble.

```{r}
#| eval: true
# Convert the Arrow object to a data frame
chartevents_df_3 <- as.data.frame(chartevents)
# Display the number of rows
cat("Number of rows:", nrow(chartevents_df_3), "\n")
# Display the first 10 rows
head(chartevents_df_3, 10)
```
